# -*- coding: utf-8 -*-
"""Housing USA (ML)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EEIiAO-IMOgca_mCtxGLkjA_bBshDlkA

#PART 1

#LINEAR REGRESSION
"""

# Commented out IPython magic to ensure Python compatibility.
#Gerekli kitaplıkları içe aktarma.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
# %matplotlib inline

import pandas as pd
import io
from google.colab import files
uploaded = files.upload()
df = pd.read_csv(io.BytesIO(uploaded['USA_Housing.csv']))
df.head()

df.shape

df.info()

df.isnull().sum()

#Ev fiyat tahmini için girdi verilerini okuma.
customers = pd.read_csv('USA_Housing.csv')
customers.head()

#Verileri açıklama.
customers.describe()

#Verilerden gelen bilgileri analiz etme.
customers.info()

#Ev fiyat tahmini verilerini görselleştirmek için grafikler.
sns.pairplot(customers)

#Veri kümesindeki tüm olası sayısal sütun kombinasyonlarını çizmek için sns.pairplot(data) kullanıyorum.
#Aşağıdaki grafiklerden, fiyatın ortalama alan geliri ile yüksek oranda ilişkili olduğu çıkarılabilir.

sns.histplot(x = df['Price']);

sns.histplot(x = df['Avg. Area Income']);

sns.histplot(x = df['Avg. Area House Age'])

sns.histplot(x = df['Avg. Area Number of Rooms']);

sns.histplot(x = df['Avg. Area Number of Bedrooms']);

sns.histplot(x = df['Area Population']);

sns.histplot(x = df['Price']);

sns.regplot(x = df['Avg. Area House Age'], y = df['Price']);

sns.regplot(x = df['Avg. Area Income'], y = df['Price']);

sns.regplot(x = df['Avg. Area Number of Rooms'], y = df['Price']);

sns.regplot(x = df['Area Population'], y = df['Price']);

sns.regplot(x = df['Avg. Area Number of Bedrooms'], y = df['Price']);

plt.figure(figsize = (15, 10))
sns.heatmap(df.corr(), annot = True, cmap = 'mako')

"""#POLYNOMIAL REGRESSION"""

X=df[['Avg. Area Income','Avg. Area House Age','Avg. Area Number of Rooms','Avg. Area Number of Bedrooms','Area Population']]
y=df['Price']

from sklearn.preprocessing import PolynomialFeatures
polynomial_converter=PolynomialFeatures(degree=3,interaction_only=False)
polynomial_features=polynomial_converter.fit(X)
polynomial_features=polynomial_converter.transform(X)
polynomial_features.shape

#Eğitim ve test için verileri bölme
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.3, random_state=101)
X_train.shape

#Modeli eğitme
from sklearn.linear_model import LinearRegression
PolyModel=LinearRegression()
PolyModel.fit(X_train,y_train)

#Test verilerini tahmin etme
y_pred=PolyModel.predict(X_test)
pd.DataFrame({'y_test':y_test,'y_pred':y_pred,'Residuals':(y_test-y_pred)}).head()

#Modeli değerlendirme
from sklearn import metrics
MAE_Poly=metrics.mean_absolute_error(y_test,y_pred)
MSE_Poly=metrics.mean_squared_error(y_test,y_pred)
RMSE_Poly=np.sqrt(MSE_Poly)

pd.DataFrame([MAE_Poly,MSE_Poly,RMSE_Poly],index=['MAE_Poly','MSE_Poly','RMSE_Poly'],columns=['Metrics'])

#Model parametrelerini ayarlama
Train_RMSE_list=[]
Test_RMSE_list=[]
for d in range(1,10):
    from sklearn.preprocessing import PolynomialFeatures
    polynomial_converter=PolynomialFeatures(degree=d,interaction_only=False)
    polynomial_features=polynomial_converter.fit(X)
    polynomial_features=polynomial_converter.transform(X)

    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.3, random_state=101)

    from sklearn.linear_model import LinearRegression
    PolyModel=LinearRegression()
    PolyModel.fit(X_train,y_train)

    y_train_pred=PolyModel.predict(X_train)
    y_test_pred=PolyModel.predict(X_test)

    from sklearn import metrics

    Train_RMSE=np.sqrt(metrics.mean_squared_error(y_train,y_train_pred))

    Test_RMSE=np.sqrt(metrics.mean_squared_error(y_test,y_test_pred))

    Train_RMSE_list.append(Train_RMSE)
    Test_RMSE_list.append(Test_RMSE)

"""Sonuç olarak, bu veri setinde lineer regresyon, polinom regresyondan daha iyi çalışır.

#RANDOM FOREST MODEL
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor

train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 42)

model = RandomForestRegressor(random_state = 1)
model.fit(train_X, train_y)
preds = model.predict(val_X)
print("MAE: ", mean_absolute_error(preds, val_y))
print("RMSE: ", np.sqrt(mean_squared_error(preds, val_y)))

"""#DECISION TREE MODEL"""

from sklearn import tree

model = tree.DecisionTreeRegressor()

model.fit(X_train,y_train)

pred = model.predict(X_test)

print_evaluate(y_test,pred)

# R2 score kontrol

tree_r2_score = r2_score(model.predict(X_test),y_test)
tree_r2_score

results_df_2 = pd.DataFrame(data=[["Decision Tree Regression", *evaluate(y_test, pred), cross_val(tree.DecisionTreeRegressor())]],
                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""# PART2"""

# Commented out IPython magic to ensure Python compatibility.
#gerekli kütüphaneleri içe aktarma
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
pd.set_option('display.max_colwidth', None)
# %matplotlib inline
plt.style.use("seaborn")
#hide warnings
import warnings
warnings.filterwarnings('ignore')

#verileri içe aktarma
import pandas as pd
import io
from google.colab import files
uploaded = files.upload()
df = pd.read_csv(io.BytesIO(uploaded['USA_Housing.csv']))

df

df.info()

df['Address'].tail()

"""##Feature Engineering (Özellik Mühendisliği)"""

def extract_address(x):
    try:
        address = x.split('\n')[1].split(',')[1].split()[0]
    except IndexError:
        address = x.split('\n')[1].split(" ")[1]
    return address

df['Address'] = df['Address'].apply(extract_address)
df

df['Address'].nunique()

"""##Univariate Data Analysis (Tek değişkenli veri analizi)

###Descriptive Statistics
"""

df.describe()

"""###Check for missing values"""

import missingno as msno
msno.matrix(df)

"""###Distribution of numerical columns"""

import random
def random_color():
    r = lambda: random.randint(0,255)
    return '#%02X%02X%02X' % (r(),r(),r())

sns.set_context("paper", font_scale=1.5, rc={"lines.linewidth": 1.8})
for i in df.columns[:-1]:
    fig,ax = plt.subplots(figsize = (6,3))
    sns.histplot(df[i],kde = True,color = random_color())

#Address count plot
fig,ax = plt.subplots(figsize = (18,6))
sns.set_theme(style="darkgrid")
ax = sns.countplot(x="Address", data=df)

"""###IQR (Inter Quartile Range)"""

for i in df.columns[:-1]:
    print(i)
    print('IQR: ',df[i].quantile(.75) - df[i].quantile(.25))
    print('-'*3)

sns.set_context("paper", font_scale=1.5, rc={"lines.linewidth": 1.8})
for i in df.columns[:-1]:
    fig,ax = plt.subplots(figsize = (9,3))
    color = np.random.rand(3,)
    ax = sns.boxplot(x=df[i],color = random_color())

"""##Bivariate Data Analysis (İki değişkenli veri analizi)

###Regression plot,Bivariate KDE,Hexbin and correlation coefficients
"""

def bivariate_plot(column,color):
    sns.set_context("paper", font_scale=1.8, rc={"lines.linewidth": 1.8})
    sns.set_style('white')
    import scipy.stats
    fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize = (21,6))
    sns.regplot(data = df,x=column, y='Price',ax = ax1,color = color,line_kws ={'color' :'red'})
    sns.kdeplot(data=df,x=column,y="Price",ax =ax2,color = color,)
    df.plot.hexbin(x=column, y='Price', gridsize=9,ax = ax3,cmap=plt.cm.Blues)
    fig.suptitle(column, fontsize=18)
    sns.despine()
    plt.show()
    print('Pearson\' correlation coefficient :',df[column].corr(df['Price']))
    print('Spearman\'s correlation coefficient :',df[column].corr(df['Price'],method = 'spearman'))
    print('Kendall\'s correlation coefficient :',df[column].corr(df['Price'],method = 'kendall'))

bivariate_plot('Avg. Area Income','#0089fa')

bivariate_plot('Avg. Area House Age','#00b032')

bivariate_plot('Avg. Area Number of Rooms','#ff5ef2')

bivariate_plot('Avg. Area Number of Bedrooms','#ff6b6b')

bivariate_plot('Area Population','#d6a400')

"""##Multivariate Data Analysis"""

sns.set_context("paper", font_scale=1.2, rc={"lines.linewidth": 1.8})
sns.pairplot(df,vars = ['Avg. Area Income',
                        'Avg. Area House Age','Avg. Area Number of Rooms',
                        'Avg. Area Number of Rooms','Area Population','Price'],kind= 'reg')

fig,ax = plt.subplots(figsize = (18,6))
sns.set_theme(style="darkgrid")
ax = sns.barplot(x="Address", y="Price", data=df,palette = 'mako')
ax.set_title('Address Vs Price',size = 18)

fig,ax = plt.subplots(figsize = (18,6))
sns.set_theme(style="darkgrid")
ax = sns.barplot(x="Address", y="Area Population", data=df,palette = 'rocket')
ax.set_title('Address Vs Area Population',size = 18)

#correlation matrix
corr = df.corr(method = 'pearson')
corr

sns.heatmap(corr,xticklabels = corr.columns,yticklabels = corr.columns,annot=True)

"""##Removing with Outliers"""

def remove_outliers(df):
    outliers = {}
    for col in df.columns:
        if str(df[col].dtype) != 'object':
            df = df[np.abs(df[col]-df[col].mean()) < (3*df[col].std())]
            olrs = df[~(np.abs(df[col]-df[col].mean()) < (3*df[col].std()))]
            outliers = pd.DataFrame(olrs)
    return df
df_outlier = remove_outliers(df)

"""##Variable Transformation (if necessary)

Variable Transformation (if necessary)
"""

def diagnostic_plots(df, variable):

    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    df[variable].hist(bins=30)
    plt.subplot(1, 2, 2)
    stats.probplot(df[variable], dist="norm", plot=plt)
    plt.suptitle(variable, fontsize=16)
    plt.show()

diagnostic_plots(df_outlier, 'Avg. Area Income')

diagnostic_plots(df_outlier, 'Avg. Area House Age')

diagnostic_plots(df_outlier, 'Avg. Area Number of Rooms')

diagnostic_plots(df_outlier, 'Area Population')

diagnostic_plots(df_outlier, 'Price')

"""All the Variavble except (Avg. Area Number of Bedrooms) are approximately normally distributed.

##Model Building
"""

y = df_outlier['Price']
#log transform target variable
y = np.log1p(y)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df_outlier.drop(['Price','Address'],axis = 1), y, test_size=0.3, random_state=0)

"""##Standard Scaling"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

"""##Measuring Performance"""

from sklearn.model_selection import cross_val_score
#create an empty dictionary to store data.
model_performance = {}

from sklearn import metrics
def print_evaluate(true,predicted):
    mae = metrics.mean_absolute_error(true, predicted)
    mse = metrics.mean_squared_error(true, predicted)
    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))
    r2_square = metrics.r2_score(true, predicted)
    print('MAE:', mae)
    print('MSE:', mse)
    print('RMSE:', rmse)
    print('R2 Square', r2_square)

"""##Visualizing"""

def plot(Model,test_pred):
    fig,ax = plt.subplots(figsize = (9,6))
    plt.scatter(y_test, test_pred, c='#0097e3',s = 6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', c='red', lw=2.7)
    plt.xlabel('Actuals')
    plt.ylabel('Predicted Values')
    plt.title('Actuals Vs Predicted Values')
    plt.suptitle(Model,fontsize = 16)
    plt.show()
    # increase size

"""##Linear Regression"""

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression(normalize=True)
lin_reg.fit(X_train_std,y_train)

test_pred = lin_reg.predict(X_test_std)
train_pred = lin_reg.predict(X_train_std)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

plot('Linear Regression',test_pred)

#getting the cross val score
score = cross_val_score(lin_reg, X_train_std, y_train, cv=9,).mean()

model_performance['Linear Regression(Simple)'] = score

"""##Lasso Regression (L1 Regularization)"""

from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
parameters = {"alpha": [0.01,0.09,0.1,1,2,5,10,100],"normalize":[True,False] ,"positive" : [True,False],
              'fit_intercept' : [True,False]}

gridsearch = GridSearchCV(Lasso(), parameters)
gridsearch.fit(X_train_std, y_train)

gridsearch.best_params_

#building the best Lasso Regression Model
lasso_reg = Lasso(alpha = 0.01,fit_intercept = True,normalize = False, positive = True)
lasso_reg.fit(X_train_std,y_train)

test_pred = lasso_reg.predict(X_test_std)
train_pred = lasso_reg.predict(X_train_std)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

plot('L1 Linear Regression',test_pred)

#getting the cross val score
score = cross_val_score(lasso_reg, X_train_std, y_train, cv=9,).mean()
model_performance['Linear Regression(Lasso)'] = score

"""##Ridge Regression (L2 Regularization)"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
parameters = {"alpha": [0.01,0.09,0.1,1,2,5,10,100],"normalize":[True,False] ,
              'fit_intercept' : [True,False]}

gridsearch = GridSearchCV(Ridge(), parameters)
gridsearch.fit(X_train_std, y_train)

gridsearch.best_params_

#Building the best Ridge mdoel
ridge_reg = Ridge(alpha = 0.1,fit_intercept = True,normalize = False)
ridge_reg.fit(X_train_std,y_train)

test_pred = ridge_reg.predict(X_test_std)
train_pred = ridge_reg.predict(X_train_std)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

plot('L2 Linear Regression',test_pred)

#getting the cross val score
score = cross_val_score(ridge_reg, X_train_std, y_train, cv=9,).mean()
model_performance['Linear Regression(Ridge)'] = score

"""##Polynomial Regrression

En iyi dereceli polinomu seçme
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import cross_val_score
degrees = [2, 3, 4, 5, 6] # Change degree "hyperparameter" here
normalizes = [True, False] # Change normalize hyperparameter here
best_score = 0
best_degree = 0
poly_reg = {
    'degree' : [],
    'normalize' : [],
    'scores' : [],
    'r2_score' : [],
}
for degree in degrees:
    for normalize in normalizes:
        poly_features = PolynomialFeatures(degree = degree)
        scaler = MinMaxScaler()

        X_train_poly = scaler.fit_transform(poly_features.fit_transform(X_train))
        X_test_poly = scaler.transform(poly_features.transform(X_test))

        polynomial_regressor = LinearRegression(normalize=normalize)
        polynomial_regressor.fit(X_train_poly, y_train)
        scores = cross_val_score(polynomial_regressor, X_train_poly, y_train, cv=6)
        test_pred = polynomial_regressor.predict(X_test_poly)

        r2_score = metrics.r2_score(y_test, test_pred)

        # Change k-fold cv value here
        poly_reg['degree'].append(degree)
        poly_reg['normalize'].append(normalize)
        poly_reg['scores'].append(scores.mean())
        poly_reg['r2_score'].append(r2_score)

pd.DataFrame(poly_reg)

"""En iyi polinom derecesi 3'tür"""

poly_reg = PolynomialFeatures(degree = 3)
from sklearn.preprocessing import PolynomialFeatures
scaler = MinMaxScaler()

#polynomial scale
poly_reg = PolynomialFeatures(degree = 3)
X_train_poly = poly_reg.fit_transform(X_train)
X_test_poly = poly_reg.transform(X_test)

#standaer scale
X_train_std_ploy = scaler.fit_transform(X_train_poly)
X_test_std_ploy = scaler.transform(X_test_poly)

ploy_reg = LinearRegression()
ploy_reg.fit(X_train_std_ploy, y_train)

test_pred = ploy_reg.predict(X_test_std_ploy)
train_pred = ploy_reg.predict(X_train_std_ploy)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

plot('Polynomial Regression',test_pred)

#getting the cross val score
score = cross_val_score(ploy_reg, X_train_std_ploy, y_train, cv=9,).mean()
model_performance['Polynomial Regression'] = score

"""##KNN Regression"""

scaler = MinMaxScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)
from sklearn.neighbors import KNeighborsRegressor

neigh = KNeighborsRegressor(n_neighbors=3)
neigh.fit(X_train_std, y_train)

test_pred = neigh.predict(X_test_std)
train_pred = neigh.predict(X_train_std)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

"""###Hyperparameter Tuning"""

parameters = {
    'n_neighbors' :np.arange(1,50),
    "weights" : ['uniform','distance'],
    "p" : [1,2],
}

gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)
gridsearch.fit(X_train_std, y_train)

gridsearch.best_params_

#en iyi KNN Regression modelini oluşturma
neigh = KNeighborsRegressor(n_neighbors=11,p = 2,weights = 'distance')
neigh.fit(X_train_std, y_train)

test_pred = neigh.predict(X_test_std)
train_pred = neigh.predict(X_train_std)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

plot('KNN Regression',test_pred)

#getting the cross val score
score = cross_val_score(neigh, X_train_std, y_train, cv=9,).mean()
model_performance['KNN Regression'] = score

"""##Support Vector Regressor (SVR)"""

from sklearn.svm import SVR
regr = SVR(kernel = 'linear')
regr.fit(X_train_std, y_train)

test_pred = regr.predict(X_test_std)
train_pred = regr.predict(X_train_std)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

"""###Hyperparameter Tuning"""

parameters = {
    'kernel' :['linear', 'poly', 'rbf', 'sigmoid'],
    'gamma' : ['scale','auto'],

}

gridsearch = GridSearchCV(SVR(), parameters)
gridsearch.fit(X_train_std, y_train)

gridsearch.best_params_

#en iyi SVR() modelini oluşturma
regr = SVR(kernel = 'rbf',gamma = 'scale')
regr.fit(X_train_std, y_train)

test_pred = regr.predict(X_test_std)
train_pred = regr.predict(X_train_std)

print("------TEST------")
print()
print_evaluate(y_test,test_pred)
print()
print("------TRAIN------")
print_evaluate(y_train,train_pred)

plot('Support vector Regressor',test_pred)

#getting the cross val score
score = cross_val_score(regr, X_train_std, y_train, cv=9,).mean()
model_performance['SVR(kernel = rbf)'] = score

"""##Visualizing the model performance"""

model_df = pd.DataFrame.from_dict(model_performance,orient = 'index',columns = ['Mean CV Score'])
model_df = model_df.sort_values(by ='Mean CV Score',ascending = False)
model_df

sns.barplot(x="Mean CV Score", y=model_df.index, data=model_df,color = '#a30023')